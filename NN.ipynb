{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед вами то, с чем я последние два раза участвовал в соревновании на Numer.ai. Numer.ai -- это сайт, куда раз в несколько дней заливают датасет с некими зашифрованными данными с фондового рынка. Что за данные -- никто не знает. Выглядят они как сотня тысяч объектов с 21 характеристикой для каждого. Характеристики нормализованы, для каждого объекта есть прогноз в виде цифры -- 0 или 1. На эти данные слетается стая дата-аналитиков, которые всячески их ковыряют, пытаются найти закономерность и построить предсказания для тестовой выборки. \n",
    "Построенные предсказания заливаются на сайт, где формируется рейтинг. \n",
    "Итак, я решил сделать нейронную сеть и посмотреть, насколько хорошо она сумеет предсказывать поведение фондового рынка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Импортируем всем известные библиотеки для работы с данными и визуализации. \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Импортируем то, что нам потребуется, из Keras. Keras представляет собой удобную обертку, позволяющую\n",
    "#в десяти строчках набросать план сети, который потом будет скомпилирован в код для Tensorflow или Theano. \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Теперь загрузим данные в фиде датафрейма Pandas.\n",
    "data = pd.read_csv('numerai_training_data.csv')\n",
    "tournament_data = pd.read_csv('numerai_tournament_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.497449</td>\n",
       "      <td>0.722213</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>0.395164</td>\n",
       "      <td>0.503541</td>\n",
       "      <td>0.301560</td>\n",
       "      <td>0.417738</td>\n",
       "      <td>0.805707</td>\n",
       "      <td>0.585739</td>\n",
       "      <td>0.538008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878240</td>\n",
       "      <td>0.477429</td>\n",
       "      <td>0.993930</td>\n",
       "      <td>0.944011</td>\n",
       "      <td>0.459007</td>\n",
       "      <td>0.838756</td>\n",
       "      <td>0.843865</td>\n",
       "      <td>0.543250</td>\n",
       "      <td>0.639446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.314430</td>\n",
       "      <td>0.442740</td>\n",
       "      <td>0.753186</td>\n",
       "      <td>0.651303</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.867722</td>\n",
       "      <td>0.662809</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.568605</td>\n",
       "      <td>0.878457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974295</td>\n",
       "      <td>0.733731</td>\n",
       "      <td>0.917947</td>\n",
       "      <td>0.913460</td>\n",
       "      <td>0.696632</td>\n",
       "      <td>0.579482</td>\n",
       "      <td>0.085998</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>0.216417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.542753</td>\n",
       "      <td>0.749818</td>\n",
       "      <td>0.490253</td>\n",
       "      <td>0.387866</td>\n",
       "      <td>0.227817</td>\n",
       "      <td>0.763056</td>\n",
       "      <td>0.297321</td>\n",
       "      <td>0.443098</td>\n",
       "      <td>0.086525</td>\n",
       "      <td>0.627223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.857279</td>\n",
       "      <td>0.042383</td>\n",
       "      <td>0.508560</td>\n",
       "      <td>0.129168</td>\n",
       "      <td>0.949880</td>\n",
       "      <td>0.325948</td>\n",
       "      <td>0.462809</td>\n",
       "      <td>0.985990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.789343</td>\n",
       "      <td>0.624176</td>\n",
       "      <td>0.766418</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.321829</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.554157</td>\n",
       "      <td>0.749399</td>\n",
       "      <td>0.462002</td>\n",
       "      <td>0.859457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298401</td>\n",
       "      <td>0.087682</td>\n",
       "      <td>0.206709</td>\n",
       "      <td>0.110735</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.775105</td>\n",
       "      <td>0.165180</td>\n",
       "      <td>0.664026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>0.990431</td>\n",
       "      <td>0.890012</td>\n",
       "      <td>0.218559</td>\n",
       "      <td>0.865423</td>\n",
       "      <td>0.968102</td>\n",
       "      <td>0.032305</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.515481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215636</td>\n",
       "      <td>0.927498</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.469240</td>\n",
       "      <td>0.257145</td>\n",
       "      <td>0.900540</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.450724</td>\n",
       "      <td>0.217831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.497449  0.722213  0.788587  0.395164  0.503541  0.301560  0.417738   \n",
       "1  0.314430  0.442740  0.753186  0.651303  0.833048  0.867722  0.662809   \n",
       "2  0.542753  0.749818  0.490253  0.387866  0.227817  0.763056  0.297321   \n",
       "3  0.789343  0.624176  0.766418  0.004252  0.321829  0.019919  0.554157   \n",
       "4  0.046066  0.815840  0.990431  0.890012  0.218559  0.865423  0.968102   \n",
       "\n",
       "   feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
       "0  0.805707  0.585739   0.538008   ...     0.878240   0.477429   0.993930   \n",
       "1  0.021921  0.568605   0.878457   ...     0.974295   0.733731   0.917947   \n",
       "2  0.443098  0.086525   0.627223   ...     0.005009   0.857279   0.042383   \n",
       "3  0.749399  0.462002   0.859457   ...     0.298401   0.087682   0.206709   \n",
       "4  0.032305  0.978947   0.515481   ...     0.215636   0.927498   0.373143   \n",
       "\n",
       "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
       "0   0.944011   0.459007   0.838756   0.843865   0.543250   0.639446       0  \n",
       "1   0.913460   0.696632   0.579482   0.085998   0.583732   0.216417       1  \n",
       "2   0.508560   0.129168   0.949880   0.325948   0.462809   0.985990       1  \n",
       "3   0.110735   0.344979   0.806925   0.775105   0.165180   0.664026       0  \n",
       "4   0.469240   0.257145   0.900540   0.236400   0.450724   0.217831       1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Данные загружены, посмотрим на первые 5 элементов обучающей выборки. \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прелестно. Как я уже говорил, нормализованные значения для каждого из 21 признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "      <td>96320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.501308</td>\n",
       "      <td>0.503716</td>\n",
       "      <td>0.501427</td>\n",
       "      <td>0.486071</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.493860</td>\n",
       "      <td>0.496498</td>\n",
       "      <td>0.503367</td>\n",
       "      <td>0.513762</td>\n",
       "      <td>0.498529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510348</td>\n",
       "      <td>0.503008</td>\n",
       "      <td>0.511927</td>\n",
       "      <td>0.506167</td>\n",
       "      <td>0.517150</td>\n",
       "      <td>0.505737</td>\n",
       "      <td>0.494409</td>\n",
       "      <td>0.499872</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>0.505170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.291842</td>\n",
       "      <td>0.292209</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.300743</td>\n",
       "      <td>0.293895</td>\n",
       "      <td>0.297529</td>\n",
       "      <td>0.289816</td>\n",
       "      <td>0.285681</td>\n",
       "      <td>0.286873</td>\n",
       "      <td>0.282871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288563</td>\n",
       "      <td>0.290839</td>\n",
       "      <td>0.294987</td>\n",
       "      <td>0.288653</td>\n",
       "      <td>0.286636</td>\n",
       "      <td>0.293463</td>\n",
       "      <td>0.284507</td>\n",
       "      <td>0.290365</td>\n",
       "      <td>0.286790</td>\n",
       "      <td>0.499976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.249180</td>\n",
       "      <td>0.255837</td>\n",
       "      <td>0.257397</td>\n",
       "      <td>0.207111</td>\n",
       "      <td>0.228888</td>\n",
       "      <td>0.237490</td>\n",
       "      <td>0.245053</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.268969</td>\n",
       "      <td>0.254813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267601</td>\n",
       "      <td>0.249391</td>\n",
       "      <td>0.259434</td>\n",
       "      <td>0.261992</td>\n",
       "      <td>0.278682</td>\n",
       "      <td>0.243173</td>\n",
       "      <td>0.258169</td>\n",
       "      <td>0.256621</td>\n",
       "      <td>0.246962</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.513009</td>\n",
       "      <td>0.500508</td>\n",
       "      <td>0.507568</td>\n",
       "      <td>0.468725</td>\n",
       "      <td>0.482481</td>\n",
       "      <td>0.491627</td>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.508466</td>\n",
       "      <td>0.522486</td>\n",
       "      <td>0.502711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528477</td>\n",
       "      <td>0.498782</td>\n",
       "      <td>0.526806</td>\n",
       "      <td>0.497373</td>\n",
       "      <td>0.530667</td>\n",
       "      <td>0.501241</td>\n",
       "      <td>0.489472</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.508131</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750271</td>\n",
       "      <td>0.760975</td>\n",
       "      <td>0.752563</td>\n",
       "      <td>0.750496</td>\n",
       "      <td>0.764957</td>\n",
       "      <td>0.761369</td>\n",
       "      <td>0.751249</td>\n",
       "      <td>0.742915</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>0.735110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761483</td>\n",
       "      <td>0.765621</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>0.754841</td>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.764578</td>\n",
       "      <td>0.739163</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.749227</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature1      feature2      feature3      feature4      feature5  \\\n",
       "count  96320.000000  96320.000000  96320.000000  96320.000000  96320.000000   \n",
       "mean       0.501308      0.503716      0.501427      0.486071      0.493400   \n",
       "std        0.291842      0.292209      0.285400      0.300743      0.293895   \n",
       "min        0.000000      0.000000      0.000140      0.000000      0.000001   \n",
       "25%        0.249180      0.255837      0.257397      0.207111      0.228888   \n",
       "50%        0.513009      0.500508      0.507568      0.468725      0.482481   \n",
       "75%        0.750271      0.760975      0.752563      0.750496      0.764957   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           feature6      feature7      feature8      feature9     feature10  \\\n",
       "count  96320.000000  96320.000000  96320.000000  96320.000000  96320.000000   \n",
       "mean       0.493860      0.496498      0.503367      0.513762      0.498529   \n",
       "std        0.297529      0.289816      0.285681      0.286873      0.282871   \n",
       "min        0.000000      0.000003      0.000000      0.000000      0.000047   \n",
       "25%        0.237490      0.245053      0.256400      0.268969      0.254813   \n",
       "50%        0.491627      0.474248      0.508466      0.522486      0.502711   \n",
       "75%        0.761369      0.751249      0.742915      0.754200      0.735110   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...          feature13     feature14     feature15     feature16  \\\n",
       "count      ...       96320.000000  96320.000000  96320.000000  96320.000000   \n",
       "mean       ...           0.510348      0.503008      0.511927      0.506167   \n",
       "std        ...           0.288563      0.290839      0.294987      0.288653   \n",
       "min        ...           0.000172      0.000000      0.000000      0.000044   \n",
       "25%        ...           0.267601      0.249391      0.259434      0.261992   \n",
       "50%        ...           0.528477      0.498782      0.526806      0.497373   \n",
       "75%        ...           0.761483      0.765621      0.776614      0.754841   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          feature17     feature18     feature19     feature20     feature21  \\\n",
       "count  96320.000000  96320.000000  96320.000000  96320.000000  96320.000000   \n",
       "mean       0.517150      0.505737      0.494409      0.499872      0.500035   \n",
       "std        0.286636      0.293463      0.284507      0.290365      0.286790   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.278682      0.243173      0.258169      0.256621      0.246962   \n",
       "50%        0.530667      0.501241      0.489472      0.493700      0.508131   \n",
       "75%        0.777363      0.764578      0.739163      0.757143      0.749227   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             target  \n",
       "count  96320.000000  \n",
       "mean       0.505170  \n",
       "std        0.499976  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не понятно. Ну и ладно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Представим обучающие и тестовые данные в виде матрицы numpy. \n",
    "data = np.asmatrix(data)\n",
    "X_tournament = np.asmatrix(tournament_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Необходимо отделить колонку с ответами от обучающей выборки. \n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Давайте категоризируем колонку с ответами. \n",
    "y = list(map(int, y))\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.49744944,  0.72221282,  0.78858684, ...,  0.84386477,\n",
       "          0.54324979,  0.63944586],\n",
       "        [ 0.31442953,  0.4427403 ,  0.75318584, ...,  0.08599803,\n",
       "          0.58373233,  0.21641711],\n",
       "        [ 0.54275321,  0.74981753,  0.49025285, ...,  0.32594778,\n",
       "          0.46280914,  0.98598974],\n",
       "        ..., \n",
       "        [ 0.73803547,  0.29122605,  0.2493957 , ...,  0.5332935 ,\n",
       "          0.74179524,  0.63491802],\n",
       "        [ 0.0861853 ,  0.44291676,  0.63376541, ...,  0.11093683,\n",
       "          0.27755192,  0.68037441],\n",
       "        [ 0.29247344,  0.79481648,  0.79898224, ...,  0.71241487,\n",
       "          0.93436928,  0.58259562]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С обучающей выборкой все в порядке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С ответами для обучающей выборки тоже все в порядке. \n",
    "Теперь делаем сеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#В нашей модели будет три слоя. В первых двух по 42 нейрона, в третьем 2. \n",
    "#В качестве борьбы с переобучением используем дропаут. \n",
    "model = Sequential()\n",
    "model.add(Dense(42, input_dim=21, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(42, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, init='uniform', activation='softmax'))\n",
    "#Сразу скомпилируем. \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Число эпох задаю через переменную для последующей визуализации.\n",
    "n = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77056 samples, validate on 19264 samples\n",
      "Epoch 1/40\n",
      "76800/77056 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5175Epoch 00000: val_loss improved from inf to 0.69254, saving model to /home/ubuntu/weights.hdf5\n",
      "77056/77056 [==============================] - 1s - loss: 0.6923 - acc: 0.5175 - val_loss: 0.6925 - val_acc: 0.5209\n",
      "Epoch 2/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5177Epoch 00001: val_loss improved from 0.69254 to 0.69211, saving model to /home/ubuntu/weights.hdf5\n",
      "77056/77056 [==============================] - 1s - loss: 0.6923 - acc: 0.5176 - val_loss: 0.6921 - val_acc: 0.5161\n",
      "Epoch 3/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5174Epoch 00002: val_loss improved from 0.69211 to 0.69193, saving model to /home/ubuntu/weights.hdf5\n",
      "77056/77056 [==============================] - 1s - loss: 0.6924 - acc: 0.5174 - val_loss: 0.6919 - val_acc: 0.5215\n",
      "Epoch 4/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5177Epoch 00003: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6922 - acc: 0.5175 - val_loss: 0.6921 - val_acc: 0.5209\n",
      "Epoch 5/40\n",
      "76800/77056 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5192Epoch 00004: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6921 - acc: 0.5192 - val_loss: 0.6927 - val_acc: 0.5210\n",
      "Epoch 6/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5182Epoch 00005: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6921 - acc: 0.5180 - val_loss: 0.6920 - val_acc: 0.5200\n",
      "Epoch 7/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5176Epoch 00006: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6921 - acc: 0.5175 - val_loss: 0.6920 - val_acc: 0.5221\n",
      "Epoch 8/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5207Epoch 00007: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6922 - acc: 0.5207 - val_loss: 0.6937 - val_acc: 0.5133\n",
      "Epoch 9/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5192Epoch 00008: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6919 - acc: 0.5191 - val_loss: 0.6963 - val_acc: 0.5209\n",
      "Epoch 10/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5191Epoch 00009: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6920 - acc: 0.5192 - val_loss: 0.6924 - val_acc: 0.5202\n",
      "Epoch 11/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5238Epoch 00010: val_loss improved from 0.69193 to 0.69179, saving model to /home/ubuntu/weights.hdf5\n",
      "77056/77056 [==============================] - 1s - loss: 0.6919 - acc: 0.5238 - val_loss: 0.6918 - val_acc: 0.5238\n",
      "Epoch 12/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.5193Epoch 00011: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6918 - acc: 0.5193 - val_loss: 0.6924 - val_acc: 0.5183\n",
      "Epoch 13/40\n",
      "76800/77056 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5215Epoch 00012: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6919 - acc: 0.5216 - val_loss: 0.6926 - val_acc: 0.5154\n",
      "Epoch 14/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5223Epoch 00013: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6920 - acc: 0.5223 - val_loss: 0.6967 - val_acc: 0.5229\n",
      "Epoch 15/40\n",
      "76800/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5219Epoch 00014: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5219 - val_loss: 0.6929 - val_acc: 0.5151\n",
      "Epoch 16/40\n",
      "76900/77056 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5194Epoch 00015: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6920 - acc: 0.5195 - val_loss: 0.6925 - val_acc: 0.5203\n",
      "Epoch 17/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5220Epoch 00016: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5221 - val_loss: 0.6922 - val_acc: 0.5211\n",
      "Epoch 18/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.5224Epoch 00017: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6918 - acc: 0.5223 - val_loss: 0.6922 - val_acc: 0.5208\n",
      "Epoch 19/40\n",
      "76500/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5207Epoch 00018: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5207 - val_loss: 0.6924 - val_acc: 0.5240\n",
      "Epoch 20/40\n",
      "76500/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5211Epoch 00019: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5211 - val_loss: 0.6924 - val_acc: 0.5186\n",
      "Epoch 21/40\n",
      "76900/77056 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5221Epoch 00020: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6915 - acc: 0.5221 - val_loss: 0.6922 - val_acc: 0.5151\n",
      "Epoch 22/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5211Epoch 00021: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5213 - val_loss: 0.6921 - val_acc: 0.5188\n",
      "Epoch 23/40\n",
      "76900/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5215Epoch 00022: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5217 - val_loss: 0.6932 - val_acc: 0.5203\n",
      "Epoch 24/40\n",
      "76500/77056 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5209Epoch 00023: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6915 - acc: 0.5208 - val_loss: 0.6936 - val_acc: 0.5179\n",
      "Epoch 25/40\n",
      "76900/77056 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5211Epoch 00024: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6915 - acc: 0.5211 - val_loss: 0.6921 - val_acc: 0.5199\n",
      "Epoch 26/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5213Epoch 00025: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5213 - val_loss: 0.6921 - val_acc: 0.5184\n",
      "Epoch 27/40\n",
      "76800/77056 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5223Epoch 00026: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6917 - acc: 0.5222 - val_loss: 0.6921 - val_acc: 0.5156\n",
      "Epoch 28/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5223Epoch 00027: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6916 - acc: 0.5224 - val_loss: 0.6937 - val_acc: 0.5207\n",
      "Epoch 29/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5231Epoch 00028: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6915 - acc: 0.5232 - val_loss: 0.6929 - val_acc: 0.5176\n",
      "Epoch 30/40\n",
      "76500/77056 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5218Epoch 00029: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5218 - val_loss: 0.6920 - val_acc: 0.5202\n",
      "Epoch 31/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5225Epoch 00030: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6914 - acc: 0.5227 - val_loss: 0.6935 - val_acc: 0.5203\n",
      "Epoch 32/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5211Epoch 00031: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6915 - acc: 0.5211 - val_loss: 0.6945 - val_acc: 0.5192\n",
      "Epoch 33/40\n",
      "77000/77056 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5223Epoch 00032: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6914 - acc: 0.5223 - val_loss: 0.6927 - val_acc: 0.5161\n",
      "Epoch 34/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5228Epoch 00033: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5230 - val_loss: 0.6922 - val_acc: 0.5195\n",
      "Epoch 35/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.5238Epoch 00034: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5238 - val_loss: 0.6931 - val_acc: 0.5174\n",
      "Epoch 36/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5241Epoch 00035: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6916 - acc: 0.5242 - val_loss: 0.6924 - val_acc: 0.5156\n",
      "Epoch 37/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5241Epoch 00036: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5240 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 38/40\n",
      "76700/77056 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5257Epoch 00037: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6914 - acc: 0.5258 - val_loss: 0.6929 - val_acc: 0.5179\n",
      "Epoch 39/40\n",
      "76900/77056 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5255Epoch 00038: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5254 - val_loss: 0.6927 - val_acc: 0.5169\n",
      "Epoch 40/40\n",
      "76600/77056 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5244Epoch 00039: val_loss did not improve\n",
      "77056/77056 [==============================] - 1s - loss: 0.6913 - acc: 0.5242 - val_loss: 0.6925 - val_acc: 0.5146\n"
     ]
    }
   ],
   "source": [
    "#Обучаем нашу сеть. Пятую часть выборки используем как валидацию. Лучшие результаты пишем в файл. \n",
    "history = model.fit(X, y,\n",
    "          batch_size=100, nb_epoch=n, verbose=1,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[ModelCheckpoint(filepath=\\\n",
    "          \"/home/ubuntu/weights.hdf5\",\\\n",
    "          verbose=1, save_best_only=True)]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Вытаскиваем историю обучения из модели. \n",
    "loss_history = history.history['loss']\n",
    "acc_history = history.history['acc']\n",
    "epochs = [(x + 1) for x in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cd75759e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Посмотрим, как она училась.\n",
    "%matplotlib inline\n",
    "ax = plt.subplot(211)\n",
    "ax.plot(epochs, loss_history, color='red')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Log loss')\n",
    "ax.set_title('Log loss per epoch')\n",
    "ax2 = plt.subplot(212)\n",
    "ax2.plot(epochs, acc_history, color='c')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy per epoch')\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вид вроде бы ничего. Перекрестная энтропия снижается, точность растет. Говорит ли это о том, что мы получим хоть сколько-нибудь приемлемый результат? Очевидно, нет, потому что на валидации качество вовсе не росло. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Берем сохраненные моделью самые лучшие веса. \n",
    "model.load_weights('/home/ubuntu/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Получаем матрицу с предсказаниями. \n",
    "result = np.hstack((X_tournament[:, 0], model.predict_proba(X_tournament[:, 1:], verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.97780000e+04,   4.80625153e-01,   5.19374847e-01],\n",
       "        [  2.14650000e+04,   4.58532095e-01,   5.41467905e-01],\n",
       "        [  2.54930000e+04,   5.32102823e-01,   4.67897177e-01],\n",
       "        ..., \n",
       "        [  2.00740000e+04,   4.24620837e-01,   5.75379193e-01],\n",
       "        [  2.67870000e+04,   5.32578230e-01,   4.67421740e-01],\n",
       "        [  1.00620000e+04,   4.57302570e-01,   5.42697430e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "#Первая колонка -- индекс объекта, вторая и третья -- вероятности, что объект принадлежит одному из классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось записать результат в файл. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['t_id', 'probability']\n",
    "dtype = [('t_id','int'), ('probability','float')]\n",
    "values = result[:, [0, 1]]\n",
    "df = pd.DataFrame(values, columns = columns)\n",
    "df[['t_id']] = df[['t_id']].astype(int)\n",
    "df.to_csv('lol.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Загружаем предсказание на сайт и убеждаемся, что модель совершенно неэффективна, а полученный нами результат никуда не годится. Log loss равен 0.69097, все равно как при случайном угадывании. В принципе, можно было потратить пять минут на случайный лес или градиентный бустинг и получить то же самое. \n",
    "Лучшие результаты модель дает при наименьшем количестве эпох -- пока она еще не сумела найти какую-то псевдозакономерность и подогнаться под нее. \n",
    "Судя по всему, лидеры рейтинга с результатом 0.64 знают о признаках то, чего не знаем мы. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
